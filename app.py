import streamlit as st
import os
import csv
from datetime import datetime

# ==========================================
# 1. æ ¸å¿ƒå¼•ç”¨
# ==========================================
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# ==========================================
# 2. é…ç½®åŒºåŸŸ
# ==========================================
os.environ["OPENAI_API_KEY"] = "sk-i0HXYWyGQZ6v5VKdoM0alDBvTpPD8GxVHja1ex6rR0lfP29G"
os.environ["OPENAI_API_BASE"] = "https://api.openai-proxy.org/v1" 

INDEX_PATH = "faiss_index_local"
LOCAL_MODEL_NAME = "shibing624/text2vec-base-chinese"
# âœ… æ—¥å¿—æ–‡ä»¶åç§°
LOG_FILE = "chat_history_log.csv"

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½
# ==========================================

@st.cache_resource
def load_embedding_model():
    """åŠ è½½æœ¬åœ°æ¨¡å‹"""
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_NAME} ...")
    return HuggingFaceEmbeddings(
        model_name=LOCAL_MODEL_NAME,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

def process_documents():
    """æ„å»ºçŸ¥è¯†åº“"""
    if not os.path.exists("./data"): return False, "âŒ æ—  data æ–‡ä»¶å¤¹"
    
    loader = DirectoryLoader('./data', glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    try: docs = loader.load()
    except Exception as e: return False, f"âŒ è¯»å–å¤±è´¥: {e}"
    if not docs: return False, "âš ï¸ data ä¸ºç©º"

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    try:
        embeddings = load_embedding_model()
        vectorstore = FAISS.from_documents(splits, embeddings)
        vectorstore.save_local(INDEX_PATH)
        return True, f"âœ… çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼"
    except Exception as e:
        return False, f"âŒ æ„å»ºå¤±è´¥: {e}"

def get_chain():
    if not os.path.exists(INDEX_PATH): return None
    
    embeddings = load_embedding_model()
    try:
        vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    except: return None

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2)
    
    # äººè®¾æç¤ºè¯
    template = """
    ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„å„¿ç§‘è¿‡æ•ä¸“ç§‘è¥å…»å¸ˆï¼Œåå«â€œæ•å®å®ˆæŠ¤è€…â€ã€‚
    è¯·åŠ¡å¿…ä¸¥æ ¼éµå®ˆä»¥ä¸‹ã€å›ç­”åŸåˆ™ã€‘ï¼š
    1. è¯­æ°”æ¸©æŸ”ã€åšå®šï¼Œå¤šç”¨â€œå’±ä»¬å®å®â€ç­‰äº²åˆ‡è¯æ±‡ã€‚
    2. ä¸¥æ ¼åŸºäºã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ã€‚
    3. å¦‚æœæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šæ³åœ°è¯´ä¸çŸ¥é“ï¼Œå¹¶å»ºè®®å’¨è¯¢åŒ»ç”Ÿã€‚

    ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
    {context}

    å®¶é•¿çš„é—®é¢˜ï¼š{question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    def format_docs(docs): return "\n\n".join(d.page_content for d in docs)

    chain = ({"context": retriever | format_docs, "question": RunnablePassthrough()} 
             | prompt | llm | StrOutputParser())
    return chain

# âœ… æ–°å¢åŠŸèƒ½ï¼šä¿å­˜èŠå¤©è®°å½•åˆ° CSV æ–‡ä»¶
def save_log(user_question, ai_answer):
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
    file_exists = os.path.isfile(LOG_FILE)
    
    with open(LOG_FILE, mode='a', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(['æ—¶é—´', 'ç”¨æˆ·é—®é¢˜', 'AIå›ç­”']) # è¡¨å¤´
        
        # å†™å…¥å½“å‰æ—¶é—´ã€é—®é¢˜ã€å›ç­”
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        writer.writerow([current_time, user_question, ai_answer])

# ==========================================
# 4. ç•Œé¢é€»è¾‘
# ==========================================
st.title("ğŸ›¡ï¸ æ•å®å®ˆæŠ¤è€…")

with st.sidebar:
    if st.button("ğŸ”„ é‡å»ºçŸ¥è¯†åº“"):
        with st.spinner("å¤„ç†ä¸­..."):
            s, m = process_documents()
            if s: st.success(m)
            else: st.error(m)
    
    # âœ… åœ¨ä¾§è¾¹æ å¢åŠ ä¸€ä¸ªä¸‹è½½æŒ‰é’®ï¼Œæ–¹ä¾¿ç®¡ç†å‘˜æŸ¥çœ‹è®°å½•
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "rb") as f:
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰èŠå¤©è®°å½•",
                data=f,
                file_name="chat_history.csv",
                mime="text/csv"
            )

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if input := st.chat_input("è¯·è¾“å…¥é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": input})
    st.chat_message("user").write(input)
    
    chain = get_chain()
    if chain:
        with st.chat_message("assistant"):
            response_container = st.empty() # åˆ›å»ºå ä½ç¬¦
            response = chain.invoke(input)
            response_container.write(response)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            
            # âœ… å…³é”®ä¸€æ­¥ï¼šè®°å½•åˆ°åå°æ–‡ä»¶
            save_log(input, response)
            print(f"ğŸ“ å·²è®°å½•: {input}") # åœ¨é»‘è‰²ç»ˆç«¯ä¹Ÿæ‰“å°ä¸€ä¸‹
    else:
        st.warning("âš ï¸ è¯·å…ˆé‡å»ºçŸ¥è¯†åº“")

